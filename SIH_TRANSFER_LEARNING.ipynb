{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kPzLWOQNybzY"
      },
      "outputs": [],
      "source": [
        "!wget https://cloud.rahulvk.com/s/dEZNYfrS5kkXM3D/download/train.zip\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://cloud.rahulvk.com/s/bNW7Hpp2dcoQgCA/download/valid.zip\n"
      ],
      "metadata": {
        "id": "YCJuPOGwyl_1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://cloud.rahulvk.com/s/S4ZMK85YGYNRjRW/download/test.zip"
      ],
      "metadata": {
        "id": "8Nzc8iOzymhW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip train.zip\n",
        "!unzip test.zip\n",
        "!unzip valid.zip"
      ],
      "metadata": {
        "id": "JqDWpClOypCE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.applications import MobileNetV2\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, GlobalAveragePooling2D\n",
        "from keras.optimizers import Adam"
      ],
      "metadata": {
        "id": "3daNMNAw0AL9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set random seeds consistently\n",
        "tf.random.set_seed(7)\n",
        "np.random.seed(7)\n",
        "\n",
        "# 0 for benign, 1 for malignant\n",
        "class_names = [\"benign\", \"malignant\"]\n",
        "\n",
        "# Define the function to generate CSV files\n",
        "def generate_csv(folder, label2int):\n",
        "    folder_name = Path(folder).name\n",
        "    labels = list(label2int.keys())\n",
        "    data_list = []\n",
        "\n",
        "    for label in labels:\n",
        "        filepaths = list(Path(folder, label).glob(\"*\"))\n",
        "        label_int = label2int[label]\n",
        "        data = [{\"filepath\": str(filepath), \"label\": str(label_int)} for filepath in filepaths]\n",
        "        data_list.extend(data)\n",
        "\n",
        "    df = pd.DataFrame(data_list)\n",
        "    output_file = f\"{folder_name}.csv\"\n",
        "    df.to_csv(output_file, index=False)\n",
        "    print(f\"Saved {output_file}\")\n",
        "\n",
        "# Generate CSV files\n",
        "generate_csv(\"train\", {\"nevus\": 0, \"seborrheic_keratosis\": 0, \"melanoma\": 1})\n",
        "generate_csv(\"valid\", {\"nevus\": 0, \"seborrheic_keratosis\": 0, \"melanoma\": 1})\n",
        "generate_csv(\"test\", {\"nevus\": 0, \"seborrheic_keratosis\": 0, \"melanoma\": 1})\n",
        "\n",
        "# Load and preprocess images using data generators\n",
        "train_metadata_filename = \"train.csv\"\n",
        "valid_metadata_filename = \"valid.csv\"\n",
        "df_train = pd.read_csv(train_metadata_filename)\n",
        "df_valid = pd.read_csv(valid_metadata_filename)\n",
        "n_training_samples = len(df_train)\n",
        "n_validation_samples = len(df_valid)\n",
        "print(\"Number of training samples:\", n_training_samples)\n",
        "print(\"Number of validation samples:\", n_validation_samples)\n",
        "\n",
        "# Data Augmentation for Training Images\n",
        "train_datagen = ImageDataGenerator(\n",
        "    rescale=1.0 / 255,\n",
        "    rotation_range=20,\n",
        "    width_shift_range=0.2,\n",
        "    height_shift_range=0.2,\n",
        "    shear_range=0.2,\n",
        "    zoom_range=0.2,\n",
        "    horizontal_flip=True,\n",
        "    fill_mode='nearest'\n",
        ")\n",
        "\n",
        "# Data Augmentation for Validation Images\n",
        "valid_datagen = ImageDataGenerator(rescale=1.0 / 255)\n",
        "\n",
        "batch_size = 32\n",
        "\n",
        "# Convert label column values to strings\n",
        "df_train['label'] = df_train['label'].astype(str)\n",
        "df_valid['label'] = df_valid['label'].astype(str)\n",
        "\n",
        "train_generator = train_datagen.flow_from_dataframe(\n",
        "    df_train,\n",
        "    x_col=\"filepath\",\n",
        "    y_col=\"label\",\n",
        "    target_size=(224, 224),  # Adjust the target size as per MobileNetV2 requirement\n",
        "    batch_size=batch_size,\n",
        "    class_mode=\"binary\",\n",
        "    shuffle=True,\n",
        "    seed=7\n",
        ")\n",
        "\n",
        "valid_generator = valid_datagen.flow_from_dataframe(\n",
        "    df_valid,\n",
        "    x_col=\"filepath\",\n",
        "    y_col=\"label\",\n",
        "    target_size=(224, 224),  # Adjust the target size as per MobileNetV2 requirement\n",
        "    batch_size=batch_size,\n",
        "    class_mode=\"binary\",\n",
        "    shuffle=False\n",
        ")\n"
      ],
      "metadata": {
        "id": "u_nug09u0EKZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Create the model\n",
        "base_model = MobileNetV2(weights='imagenet', include_top=False)\n",
        "model = Sequential()\n",
        "model.add(base_model)\n",
        "model.add(GlobalAveragePooling2D())\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer=Adam(lr=0.0001), loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(train_generator, validation_data=valid_generator, epochs=10)\n",
        "\n",
        "# Save the model\n",
        "save_model_path = 'LatestModel.h5'  # Replace with the desired path to save the model\n",
        "model.save(save_model_path)"
      ],
      "metadata": {
        "id": "pH2Bw3Hv0Et4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_image_class(img_path, model, threshold=0.5):\n",
        "  img = tf.keras.preprocessing.image.load_img(img_path, target_size=(299, 299))\n",
        "  img = tf.keras.preprocessing.image.img_to_array(img)\n",
        "  img = tf.expand_dims(img, 0) # Create a batch\n",
        "  img = tf.keras.applications.inception_v3.preprocess_input(img)\n",
        "  img = tf.image.convert_image_dtype(img, tf.float32)\n",
        "  predictions = model.predict(img)\n",
        "  score = predictions.squeeze()\n",
        "  if score >= threshold:\n",
        "    print(f\"This image is {100 * score:.2f}% malignant.\")\n",
        "  else:\n",
        "    print(f\"This image is {100 * (1 - score):.2f}% benign.\")\n",
        "  plt.imshow(img[0])\n",
        "  plt.axis('off')\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "iiz5-2B_0LSU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# loading data\n",
        "train_metadata_filename = \"train.csv\"\n",
        "valid_metadata_filename = \"valid.csv\"\n",
        "# load CSV files as DataFrames\n",
        "df_train = pd.read_csv(train_metadata_filename)\n",
        "df_valid = pd.read_csv(valid_metadata_filename)\n",
        "n_training_samples = len(df_train)\n",
        "n_validation_samples = len(df_valid)\n",
        "print(\"Number of training samples:\", n_training_samples)\n",
        "print(\"Number of validation samples:\", n_validation_samples)\n",
        "train_ds = tf.data.Dataset.from_tensor_slices((df_train[\"filepath\"], df_train[\"label\"]))\n",
        "valid_ds = tf.data.Dataset.from_tensor_slices((df_valid[\"filepath\"], df_valid[\"label\"]))"
      ],
      "metadata": {
        "id": "4-ukuNje0Qav"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# preprocess data\n",
        "def decode_img(img):\n",
        "  # convert the compressed string to a 3D uint8 tensor\n",
        "  img = tf.image.decode_jpeg(img, channels=3)\n",
        "  # Use `convert_image_dtype` to convert to floats in the [0,1] range.\n",
        "  img = tf.image.convert_image_dtype(img, tf.float32)\n",
        "  # resize the image to the desired size.\n",
        "  return tf.image.resize(img, [299, 299])\n",
        "\n",
        "\n",
        "def process_path(filepath, label):\n",
        "  # load the raw data from the file as a string\n",
        "  img = tf.io.read_file(filepath)\n",
        "  img = decode_img(img)\n",
        "  return img, label\n",
        "\n",
        "\n",
        "valid_ds = valid_ds.map(process_path)\n",
        "train_ds = train_ds.map(process_path)\n",
        "# test_ds = test_ds\n",
        "for image, label in train_ds.take(1):\n",
        "    print(\"Image shape:\", image.shape)\n",
        "    print(\"Label:\", label.numpy())"
      ],
      "metadata": {
        "id": "pDkiQO020Slu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# training parameters\n",
        "batch_size = 64\n",
        "optimizer = \"rmsprop\"\n",
        "\n",
        "def prepare_for_training(ds, cache=True, batch_size=64, shuffle_buffer_size=1000):\n",
        "  if cache:\n",
        "    if isinstance(cache, str):\n",
        "      ds = ds.cache(cache)\n",
        "    else:\n",
        "      ds = ds.cache()\n",
        "  # shuffle the dataset\n",
        "  ds = ds.shuffle(buffer_size=shuffle_buffer_size)\n",
        "  # Repeat forever\n",
        "  ds = ds.repeat()\n",
        "  # split to batches\n",
        "  ds = ds.batch(batch_size)\n",
        "  # `prefetch` lets the dataset fetch batches in the background while the model\n",
        "  # is training.\n",
        "  ds = ds.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
        "  return ds\n",
        "\n",
        "valid_ds = prepare_for_training(valid_ds, batch_size=batch_size, cache=\"valid-cached-data\")\n",
        "train_ds = prepare_for_training(train_ds, batch_size=batch_size, cache=\"train-cached-data\")"
      ],
      "metadata": {
        "id": "lrkkk3O10UzJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# evaluation\n",
        "# load testing set\n",
        "test_metadata_filename = \"test.csv\"\n",
        "df_test = pd.read_csv(test_metadata_filename)\n",
        "n_testing_samples = len(df_test)\n",
        "print(\"Number of testing samples:\", n_testing_samples)\n",
        "test_ds = tf.data.Dataset.from_tensor_slices((df_test[\"filepath\"], df_test[\"label\"]))\n",
        "\n",
        "def prepare_for_testing(ds, cache=True, shuffle_buffer_size=1000):\n",
        "  if cache:\n",
        "    if isinstance(cache, str):\n",
        "      ds = ds.cache(cache)\n",
        "    else:\n",
        "      ds = ds.cache()\n",
        "  ds = ds.shuffle(buffer_size=shuffle_buffer_size)\n",
        "  return ds\n",
        "\n",
        "test_ds = test_ds.map(process_path)\n",
        "test_ds = prepare_for_testing(test_ds, cache=\"test-cached-data\")"
      ],
      "metadata": {
        "id": "oglhIHYr0XMu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# convert testing set to numpy array to fit in memory (don't do that when testing\n",
        "# set is too large)\n",
        "y_test = np.zeros((n_testing_samples,))\n",
        "X_test = np.zeros((n_testing_samples, 299, 299, 3))\n",
        "for i, (img, label) in enumerate(test_ds.take(n_testing_samples)):\n",
        "  # print(img.shape, label.shape)\n",
        "  X_test[i] = img\n",
        "  y_test[i] = label.numpy()\n",
        "\n",
        "print(\"y_test.shape:\", y_test.shape)"
      ],
      "metadata": {
        "id": "X4ev3qw10ag6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Evaluating the model...\")\n",
        "loss, accuracy = model.evaluate(X_test, y_test, verbose=0)\n",
        "print(\"Loss:\", loss, \"  Accuracy:\", accuracy)"
      ],
      "metadata": {
        "id": "U6D2hwq00cwY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from keras.models import load_model\n",
        "from PIL import Image\n",
        "from keras.preprocessing import image\n",
        "\n",
        "# Load the saved model for prediction\n",
        "loaded_model = load_model('LatestModel.h5')\n",
        "\n",
        "# Load an image for prediction (replace 'path_to_image.jpg' with the actual image path)\n",
        "#img_path = 'ISIC_2019_Training_Input/ISIC_0000002.jpg'\n",
        "img_path = 'sc2.jpeg'\n",
        "img = Image.open(img_path)\n",
        "img = img.resize((224, 224))  # Resize the image to match the model's input size\n",
        "\n",
        "# Convert the image to a numpy array\n",
        "img_array = np.array(img)\n",
        "img_array = np.expand_dims(img_array, axis=0)\n",
        "\n",
        "# Make a prediction\n",
        "prediction = loaded_model.predict(img_array)\n",
        "print(prediction)\n",
        "# Convert the prediction to a class label\n",
        "if prediction[0][0] >= 0.4:\n",
        "    print(\"Malignant\")\n",
        "else:\n",
        "    print(\"Benign\")\n"
      ],
      "metadata": {
        "id": "Zst5mArG0Lh6"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}